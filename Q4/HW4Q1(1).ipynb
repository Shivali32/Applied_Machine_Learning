{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applied Machine Learning Homework 4 Question 1 \n",
    "\n",
    "### Group 111\n",
    "<br>Aryan Dhuru\n",
    "<br>Saloni Gandhi\n",
    "<br>Mitanshu Bhoot\n",
    "<br>Shivali Mate\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CLIP (Contrastive Language-Image Pre-training)\n",
    "\n",
    "Source: openai/clip-vit-base-patch32\n",
    "\n",
    "### 1. Architecture\n",
    "\n",
    "- **Image Encoder**:\n",
    "    - **ResNet**:\n",
    "    Includes variants like ResNet-50, ResNet-101, and scaled versions (e.g., RN50x64).\n",
    "    Features ResNet-D enhancements, antialiased pooling, and attention pooling.\n",
    "    - **Vision Transformer (ViT)**: The model uses a Vision Transformer as its backbone for processing images. Uses variants such as ViT-B/32 (86M parameters) and ViT-L/14 (307M parameters).\n",
    "\n",
    "- **Text Encoder**: \n",
    "    - The text encoder is based on the GPT-2 architecture.\n",
    "    - The input text is tokenized and embedded into vectors using a learned embedding layer.\n",
    "    - Positional Encoding: Like the ViT, positional encodings are added to the token embeddings to retain the sequence information.\n",
    "\n",
    "\n",
    "- **Shared Embedding Space**:\n",
    "    - **Projection Layers**: \n",
    "    After the vision and text encoders process their respective inputs, the resulting embeddings are projected into a shared space using separate linear projection layers for images and text.\n",
    "    - **Contrastive Learning**: \n",
    "    During training, CLIP uses a contrastive loss to align the image and text embeddings in this shared space. Matching pairs (image and its corresponding caption) are brought closer together, while non-matching pairs are pushed apart.\n",
    "\n",
    "### 2. Number of Layers and Parameters\n",
    "\n",
    "- **Vision Transformer (ViT)**: \n",
    "    - The Vision Transformer used in CLIP has 12 layers with 512 hidden units and 8 attention heads.\n",
    "    - Typical parameter count: ~23M for ResNet-50.\n",
    "- **Text Encoder (GPT-2)**: \n",
    "    - The text encoder used in CLIP has 24 layers.\n",
    "    - Typical parameter count: ~63M.\n",
    "\n",
    "### Parameters Breakdown\n",
    "\n",
    "- **Embedding Layer**: Converts input images/text into embeddings.\n",
    "- **Transformer Layers**: Each layer consists of multi-head self-attention mechanisms and feed-forward neural networks.\n",
    "- **Parameters**: The parameters include weights for the attention mechanism, feed-forward networks, and normalization layers.\n",
    "- **K, Q, V Matrices**: These are part of the self-attention mechanism, where K (Key), Q (Query), and V (Value) matrices are used to compute attention scores.\n",
    "\n",
    "### 3. Functionality\n",
    "\n",
    "- **Image and Text Embeddings**: The model learns to map images and text into a shared embedding space.\n",
    "- **Contrastive Loss**: The model is trained using a contrastive loss function, which encourages the embeddings of matching image-text pairs to be closer together, while non-matching pairs are pushed apart.\n",
    "- **Zero-Shot Transfer**: After pre-training, the model can perform tasks without additional training by leveraging natural language descriptions.\n",
    "\n",
    "### 4. Training and Objectives\n",
    "- The model is trained on a dataset of 400M image-text pairs using a contrastive loss to maximize similarity between correct pairs and minimize similarity for incorrect ones.\n",
    "- Zero-shot transfer is achieved by encoding class names or descriptions as text embeddings and comparing them to image embeddings.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating blurred/airplane: 100%|██████████| 2/2 [00:00<00:00,  3.55it/s]\n",
      "Evaluating blurred/car: 100%|██████████| 2/2 [00:00<00:00,  5.15it/s]\n",
      "Evaluating blurred/chair: 100%|██████████| 1/1 [00:00<00:00,  4.80it/s]\n",
      "Evaluating blurred/cup: 0it [00:00, ?it/s]\n",
      "Evaluating blurred/dog: 0it [00:00, ?it/s]\n",
      "Evaluating blurred/donkey: 0it [00:00, ?it/s]\n",
      "Evaluating blurred/duck: 0it [00:00, ?it/s]\n",
      "Evaluating blurred/hat: 0it [00:00, ?it/s]\n",
      "Evaluating features/airplane: 100%|██████████| 1/1 [00:00<00:00,  4.56it/s]\n",
      "Evaluating features/car: 100%|██████████| 1/1 [00:00<00:00,  5.79it/s]\n",
      "Evaluating features/chair: 100%|██████████| 1/1 [00:00<00:00,  5.05it/s]\n",
      "Evaluating features/cup: 0it [00:00, ?it/s]\n",
      "Evaluating features/dog: 0it [00:00, ?it/s]\n",
      "Evaluating features/donkey: 0it [00:00, ?it/s]\n",
      "Evaluating features/duck: 0it [00:00, ?it/s]\n",
      "Evaluating features/hat: 0it [00:00, ?it/s]\n",
      "Evaluating geons/airplane: 0it [00:00, ?it/s]\n",
      "Evaluating geons/car: 0it [00:00, ?it/s]\n",
      "Evaluating geons/chair: 0it [00:00, ?it/s]\n",
      "Evaluating geons/cup: 100%|██████████| 1/1 [00:00<00:00,  4.87it/s]\n",
      "Evaluating geons/dog: 100%|██████████| 3/3 [00:00<00:00,  4.67it/s]\n",
      "Evaluating geons/donkey: 100%|██████████| 4/4 [00:00<00:00,  5.53it/s]\n",
      "Evaluating geons/duck: 0it [00:00, ?it/s]\n",
      "Evaluating geons/hat: 100%|██████████| 3/3 [00:00<00:00,  5.46it/s]\n",
      "Evaluating realistic/airplane: 0it [00:00, ?it/s]\n",
      "Evaluating realistic/car: 0it [00:00, ?it/s]\n",
      "Evaluating realistic/chair: 0it [00:00, ?it/s]\n",
      "Evaluating realistic/cup: 0it [00:00, ?it/s]\n",
      "Evaluating realistic/dog: 0it [00:00, ?it/s]\n",
      "Evaluating realistic/donkey: 0it [00:00, ?it/s]\n",
      "Evaluating realistic/duck: 0it [00:00, ?it/s]\n",
      "Evaluating realistic/hat: 0it [00:00, ?it/s]\n",
      "Evaluating silhouettes/airplane: 0it [00:00, ?it/s]\n",
      "Evaluating silhouettes/car: 0it [00:00, ?it/s]\n",
      "Evaluating silhouettes/chair: 100%|██████████| 1/1 [00:00<00:00,  5.31it/s]\n",
      "Evaluating silhouettes/cup: 0it [00:00, ?it/s]\n",
      "Evaluating silhouettes/dog: 100%|██████████| 1/1 [00:00<00:00,  5.32it/s]\n",
      "Evaluating silhouettes/donkey: 0it [00:00, ?it/s]\n",
      "Evaluating silhouettes/duck: 0it [00:00, ?it/s]\n",
      "Evaluating silhouettes/hat: 100%|██████████| 1/1 [00:00<00:00,  4.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for blurred: 80.00%\n",
      "Accuracy for features: 66.67%\n",
      "Accuracy for geons: 36.36%\n",
      "Accuracy for realistic: 0.00%\n",
      "Accuracy for silhouettes: 100.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Set up device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Load the CLIP model\n",
    "model_id = \"openai/clip-vit-base-patch32\"\n",
    "model = CLIPModel.from_pretrained(model_id).to(device)\n",
    "processor = CLIPProcessor.from_pretrained(model_id)\n",
    "\n",
    "# Define the dataset directory and categories\n",
    "data_dir = \"./dataset\"  # Replace with your dataset directory\n",
    "categories = [\"airplane\", \"car\", \"chair\", \"cup\", \"dog\", \"donkey\", \"duck\", \"hat\"]\n",
    "conditions = [\"blurred\", \"features\", \"geons\", \"realistic\", \"silhouettes\"]\n",
    "\n",
    "# def evaluate_model(data_dir, categories, conditions):\n",
    "#     accuracy = {condition: 0 for condition in conditions}\n",
    "    \n",
    "#     for condition in conditions:\n",
    "#         correct = 0\n",
    "#         total = 0\n",
    "#         condition_dir = os.path.join(data_dir, condition)\n",
    "        \n",
    "#         for category in categories:\n",
    "#             category_images = [f for f in os.listdir(condition_dir) if f.startswith(category)]\n",
    "            \n",
    "#             for img_name in tqdm(category_images, desc=f\"Evaluating {condition}/{category}\"):\n",
    "#                 img_path = os.path.join(condition_dir, img_name)\n",
    "#                 image = Image.open(img_path).convert(\"RGB\")\n",
    "                \n",
    "#                 inputs = processor(text=categories, images=image, return_tensors=\"pt\", padding=True)\n",
    "#                 inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "#                 outputs = model(**inputs)\n",
    "                \n",
    "#                 logits_per_image = outputs.logits_per_image\n",
    "#                 probs = logits_per_image.softmax(dim=-1)\n",
    "                \n",
    "#                 # Get the predicted category\n",
    "#                 pred = torch.argmax(probs).item()\n",
    "#                 if categories[pred] == category:\n",
    "#                     correct += 1\n",
    "#                 total += 1\n",
    "        \n",
    "#         # Avoid division by zero\n",
    "#         if total > 0:\n",
    "#             accuracy[condition] = correct / total\n",
    "#         else:\n",
    "#             accuracy[condition] = 0  # Assign 0 accuracy if no images were processed\n",
    "    \n",
    "#     return accuracy\n",
    "\n",
    "# Function to evaluate the model\n",
    "def evaluate_model(data_dir, categories, conditions):\n",
    "    accuracy = {condition: 0 for condition in conditions}\n",
    "    total_images = {condition: 0 for condition in conditions}  # New dictionary to store the count of images\n",
    "    \n",
    "    for condition in conditions:\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        condition_dir = os.path.join(data_dir, condition)\n",
    "        \n",
    "        for category in categories:\n",
    "            category_images = [f for f in os.listdir(condition_dir) if f.startswith(category)]\n",
    "            \n",
    "            for img_name in tqdm(category_images, desc=f\"Evaluating {condition}/{category}\"):\n",
    "                img_path = os.path.join(condition_dir, img_name)\n",
    "                image = Image.open(img_path).convert(\"RGB\")\n",
    "                \n",
    "                inputs = processor(text=categories, images=image, return_tensors=\"pt\", padding=True)\n",
    "                inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "                outputs = model(**inputs)\n",
    "                \n",
    "                logits_per_image = outputs.logits_per_image\n",
    "                probs = logits_per_image.softmax(dim=-1)\n",
    "                \n",
    "                # Get the predicted category\n",
    "                pred = torch.argmax(probs).item()\n",
    "                if categories[pred] == category:\n",
    "                    correct += 1\n",
    "                total += 1\n",
    "        \n",
    "        accuracy[condition] = correct / total if total > 0 else 0\n",
    "        total_images[condition] = total  # Store the total number of images evaluated\n",
    "    \n",
    "    return accuracy, total_images\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy, total_images = evaluate_model(data_dir, categories, conditions)\n",
    "\n",
    "# Print the accuracy and number of images evaluated\n",
    "for condition, acc in accuracy.items():\n",
    "    print(f\"Accuracy for {condition}: {acc * 100:.2f}%\")\n",
    "    print(f\"Number of images evaluated for {condition}: {total_images[condition]}\")\n",
    "\n",
    "\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = evaluate_model(data_dir, categories, conditions)\n",
    "\n",
    "# Print the accuracy\n",
    "for condition, acc in accuracy.items():\n",
    "    print(f\"Accuracy for {condition}: {acc * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
