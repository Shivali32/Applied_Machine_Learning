{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import seaborn as sns\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # Load the dataset into a DataFrame\n",
    "# # Assuming df is your DataFrame\n",
    "\n",
    "# # Step 1: Compute Pearson Correlation Coefficient (PCC) for numeric columns\n",
    "# numeric_columns = ['age', 'final.weight', 'education.num', 'capital.gain', 'capital.loss', 'hours.per.week']\n",
    "\n",
    "# # Compute correlation matrix for numeric columns\n",
    "# correlation_matrix = df[continuous_features].corr()\n",
    "\n",
    "# # Display the correlation matrix\n",
    "# print(\"Pearson Correlation Coefficient (PCC) matrix:\")\n",
    "# print(correlation_matrix)\n",
    "\n",
    "# # Step 2: Generate scatter plots for numeric columns against the label (e.g., income)\n",
    "# # First, we must convert 'income' to numeric values for correlation analysis\n",
    "# df['income_numeric'] = df['income'].apply(lambda x: 1 if x == '>50K' else 0)\n",
    "\n",
    "# # Plot scatter plots\n",
    "# for col in numeric_columns:\n",
    "#     plt.figure(figsize=(6, 4))\n",
    "#     sns.scatterplot(data=df, x=col, y='income_numeric')\n",
    "#     plt.title(f'Scatter Plot of {col} vs Income')\n",
    "#     plt.xlabel(col)\n",
    "#     plt.ylabel('Income (>50K = 1, <=50K = 0)')\n",
    "#     plt.show()\n",
    "\n",
    "# # Step 3: Analyze categorical variables\n",
    "# categorical_columns = ['workclass', 'education', 'marital.status', 'occupation', 'relationship', 'race', 'sex', 'native.country']\n",
    "\n",
    "# # Bar plots for categorical variables\n",
    "# for col in categorical_features:\n",
    "#     plt.figure(figsize=(10, 6))\n",
    "#     sns.countplot(data=df, x=col, hue='income')\n",
    "#     plt.xticks(rotation=90)  # Rotate x-axis labels for better readability\n",
    "#     plt.title(f'Income Distribution by {col}')\n",
    "#     plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Fit the model\n",
    "# lr_pipeline.fit(X_train, y_train)\n",
    "\n",
    "# # Evaluate using cross-validation\n",
    "# cv_scores = cross_val_score(lr_pipeline, X_train, y_train, cv=3)\n",
    "# print(\"Linear Regression cross-validation score: \", cv_scores)\n",
    "\n",
    "# # Perform prediction and evaluate the model\n",
    "# y_pred = lr_pipeline.predict(X_test)\n",
    "# mse = mean_squared_error(y_test, y_pred)\n",
    "# print(f'Mean Squared Error on test set: {mse}')\n",
    "\n",
    "\n",
    "# preprocessor = ColumnTransformer(\n",
    "#     transformers=[\n",
    "#         ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_cols)\n",
    "#     ], remainder='passthrough')  # Keeps numerical columns unchanged\n",
    "\n",
    "# # Ridge Regression pipeline\n",
    "# ridge_pipeline = Pipeline(steps=[\n",
    "#     ('preprocessor', preprocessor),\n",
    "#     ('model', Ridge(alpha=1.0))  # Alpha can be tuned\n",
    "# ])\n",
    "\n",
    "# # Lasso Regression pipeline\n",
    "# lasso_pipeline = Pipeline(steps=[\n",
    "#     ('preprocessor', preprocessor),\n",
    "#     ('model', Lasso(alpha=0.1))  # Alpha can be tuned\n",
    "# ])\n",
    "\n",
    "# # ElasticNet Regression pipeline\n",
    "# elasticnet_pipeline = Pipeline(steps=[\n",
    "#     ('preprocessor', preprocessor),\n",
    "#     ('model', ElasticNet(alpha=0.1, l1_ratio=0.5))  # Alpha and l1_ratio can be tuned\n",
    "# ])\n",
    "\n",
    "# # SGDRegressor pipeline\n",
    "# sgd_pipeline = Pipeline(steps=[\n",
    "#     ('preprocessor', preprocessor),\n",
    "#     ('model', SGDRegressor(max_iter=1000, penalty='l2', learning_rate='adaptive'))  # L2 regularization\n",
    "# ])\n",
    "\n",
    "# # List of pipelines for the models\n",
    "# models = [\n",
    "#     ('Ridge', ridge_pipeline),\n",
    "#     ('Lasso', lasso_pipeline),\n",
    "#     ('ElasticNet', elasticnet_pipeline),\n",
    "#     ('SGD', sgd_pipeline)\n",
    "# ]\n",
    "\n",
    "# # Train and evaluate each model using cross-validation and display the results\n",
    "# for name, model in models:\n",
    "#     print(f\"\\n{name} Regression\")\n",
    "#     model.fit(X_train, y_train)  # Train the model\n",
    "#     cv_scores = cross_val_score(model, X_train, y_train, cv=3)  # 3-fold cross-validation\n",
    "#     print(f\"Cross-validation score: {cv_scores.mean()}\")\n",
    "\n",
    "#     # Predict and evaluate on the test set\n",
    "#     y_pred = model.predict(X_test)\n",
    "#     mse = mean_squared_error(y_test, y_pred)\n",
    "#     print(f\"Mean Squared Error on test set: {mse}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After running the SGD training with different learning rates and batch sizes, you would typically observe the following trends:\n",
    "\n",
    "Learning Rate Impact:\n",
    "\n",
    "Low Learning Rate (e.g., 0.001): The model may converge slowly, leading to higher MSE initially but could eventually settle at a good minimum. This is especially true if the number of iterations is sufficient.\n",
    "Medium Learning Rate (e.g., 0.01): Often strikes a balance between convergence speed and stability, usually resulting in lower MSE.\n",
    "High Learning Rate (e.g., 0.1): The model may overshoot the optimal point, leading to oscillations and potentially higher MSE values.\n",
    "Batch Size Impact:\n",
    "\n",
    "Small Batch Size (e.g., 1): Provides noisy gradients, which may help the model escape local minima but can lead to fluctuations in loss. This may also result in longer training times.\n",
    "Medium Batch Size (e.g., 10): Typically offers a balance between noisy updates and stable convergence, often resulting in lower MSE.\n",
    "Large Batch Size (e.g., 20): More accurate gradient estimates but fewer updates per epoch, which may lead to slower convergence and sometimes higher MSE.\n",
    "General Observations:\n",
    "\n",
    "A combination of a moderate learning rate with an appropriate batch size (like 10) tends to yield the best performance in terms of minimizing MSE.\n",
    "An analysis of the plots would also reveal that as batch size increases, the MSE tends to stabilize for the given learning rates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Findings and Analysis\n",
    "Cross-Validation MSE: The closed-form solution using polynomial regression will give you a baseline MSE, which you can compare against SGD and regularized models.\n",
    "\n",
    "Impact of Regularization:\n",
    "\n",
    "Ridge Regression: Generally performs well with polynomial features as it mitigates overfitting by shrinking coefficients.\n",
    "Lasso Regression: Can eliminate less important features entirely, which may be beneficial for models with many polynomial terms.\n",
    "Elastic Net: Combines both Lasso and Ridge benefits and can be particularly useful if features are highly correlated.\n",
    "Impact of Hyperparameters:\n",
    "\n",
    "Learning Rate:\n",
    "A low learning rate might result in slow convergence, while a high learning rate might cause the model to diverge.\n",
    "Batch Size:\n",
    "Smaller batch sizes provide more updates and can lead to faster convergence, while larger batch sizes result in fewer updates but may stabilize learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.9568,\n",
    "1.9586,\n",
    "1.9417,\n",
    "1.9417,\n",
    "1.9417,\n",
    "1.9857,\n",
    "1.9965,\n",
    "2.1716,\n",
    "1.9611,\n",
    "1.9885,\n",
    "2.1139,\n",
    "2.4279,\n",
    "3.3553"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
